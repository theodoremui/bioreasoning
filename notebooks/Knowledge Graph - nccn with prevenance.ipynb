{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5202b22d",
   "metadata": {},
   "source": [
    "# Knowledge Graphs - NCCN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2102cff3",
   "metadata": {},
   "source": [
    "## 1. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11302225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9140563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dd68e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Any, List, Callable, Optional, Union, Dict, Tuple\n",
    "LLAMACLOUD_API_KEY=os.getenv(\"LLAMACLOUD_API_KEY\")\n",
    "len(LLAMACLOUD_API_KEY)\n",
    "\n",
    "# Global constants\n",
    "NCCN_COMMUNITIES_CACHE_FILE = \"data/nccn_communities.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df9c38f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b8843e",
   "metadata": {},
   "source": [
    "## 2. Loading & Parsing Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fec320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cloud_services.parse import LlamaParse\n",
    "# Initialize parser with specified mode\n",
    "parser = LlamaParse(\n",
    "    api_key=LLAMACLOUD_API_KEY,\n",
    "    num_workers=4,\n",
    "    verbose=True,\n",
    "    language=\"en\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3705f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 9e53ec42-1d51-4b99-8d6e-7c189459a84e\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the PDF file to parse\n",
    "pdf_path = \"../data/nccn_breast_cancer.pdf\"\n",
    "\n",
    "# Parse the document asynchronously\n",
    "results = await parser.aparse(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "388d77f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f3c2507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Printed by Stina Singel on 6/17/2025 3:05:05 AM. For personal use only. Not approved for distribution. Copyright © 2025 National Comprehensive Cancer Network, Inc., All Rights Reserved.\\n\\n                       NCCN Guidelines Version 4.2025                                                                                                               NCCN Guidelines Index\\n                       Invasive Breast Cancer                                                                                                                           Table of Contents\\n                                                                                                                                                                                         Discussion\\n\\n                                 TARGETED THERAPIES AND ASSOCIATED BIOMARKER TESTING\\n                       FOR RECURRENT UNRESECTABLE (LOCAL OR REGIONAL) OR STAGE IV (M1) DISEASE\\n\\n                                                      REFERENCE'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page100 = results.pages[100]\n",
    "page100.text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f59d1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPrinted by Stina Singel on 6/17/2025 3:05:05 AM. For personal use only. Not approved for distribution. Copyright © 2025 National Comprehensive Cancer Network, Inc., All Rights Reserved.\\n\\n# NCCN Guidelines Version 4.2025\\n\\n# NCCN Guidelines Index\\n\\n# Invasive Breast Cancer\\n\\n# Table of Contents\\n\\n# Discussion\\n\\n# TARGETED THERAPIES AND ASSOCIATED BIOMARKER TESTING FOR RECURRENT UNRESECTABLE (LOCAL OR REGIONAL) OR STAGE IV (M1) DISEASE\\n\\n# REFERENCES\\n\\n1. Andre F, Ciruelos E, Rubovszky G, et al. Alpelisib for PIK3CA-mutated, hormone receptor-positive advanced breast cancer. N Engl J Med 2019;380:1929-1940.\\n2. Turner NC, Oliveira M, Howell SJ, et al. Capivasertib in hormone receptor–positive advanced breast cancer. N Engl J Med 2023;388:2058-2070.\\n3. Berton D, Banerjee S, Curigliano G, et al. Antitumor activity of dostarlimab in patients with mismatch repair–deficient (dMMR) tumors: a combined analysis of 2 cohorts in the GARNET study. Poster presented at American Society for Clinical Oncology '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page100.md[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a827b4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of results.pages: <class 'list'>\n",
      "Number of pages: 263\n",
      "Type of first page: <class 'llama_cloud_services.parse.types.Page'>\n",
      "Page attributes: ['charts', 'construct', 'copy', 'dict', 'from_orm', 'height', 'images', 'items', 'json', 'layout', 'links', 'md', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'noStructuredContent', 'noTextContent', 'page', 'parse_file', 'parse_obj', 'parse_raw', 'parsingMode', 'schema', 'schema_json', 'status', 'structuredData', 'tables', 'text', 'triggeredAutoMode', 'update_forward_refs', 'validate', 'width']\n",
      "✓ Successfully cleaned zip_text from all pages\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'                       NCCN Guidelines Version 4.2025                                                                                                               NCCN Guidelines Index\\n                       Invasive Breast Cancer                                                                                                                           Table of Contents\\n                                                                                                                                                                                         Discussion\\n\\n                                 TARGETED THERAPIES AND ASSOCIATED BIOMARKER TESTING\\n                       FOR RECURRENT UNRESECTABLE (LOCAL OR REGIONAL) OR STAGE IV (M1) DISEASE\\n\\n                                                      REFERENCES\\n\\n 1 Andre F, Ciruelos E, Rubovszky G, et al. Alpelisib for PIK3CA-mutated, hormone receptor-positive advanced breast cancer. N Engl J Med 2019;380:1929-1940.\\n 2 Turner NC, Oliveira M, H'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_text = \"Printed by Stina Singel on 6/17/2025 3:05:05 AM. For personal use only. Not approved for distribution. Copyright © 2025 National Comprehensive Cancer Network, Inc., All Rights Reserved.\\n\\n\"\n",
    "\n",
    "print(f\"Type of results.pages: {type(results.pages)}\")\n",
    "print(f\"Number of pages: {len(results.pages)}\")\n",
    "if results.pages:\n",
    "    print(f\"Type of first page: {type(results.pages[0])}\")\n",
    "    print(f\"Page attributes: {[attr for attr in dir(results.pages[0]) if not attr.startswith('_')]}\")\n",
    "\n",
    "# CORRECT way: Modify the text attribute of each page object in-place\n",
    "for page in results.pages:\n",
    "    if hasattr(page, 'text') and page.text:\n",
    "        page.text = page.text.replace(zip_text, \"\")\n",
    "    if hasattr(page, 'md') and page.md:\n",
    "        page.md = page.md.replace(zip_text, \"\")\n",
    "\n",
    "print(\"✓ Successfully cleaned zip_text from all pages\")\n",
    "\n",
    "# Now test that the page object still works\n",
    "results.pages[100].text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffcc43f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# NCCN Guidelines Version 4.2025\\n\\n# NCCN Guidelines Index\\n\\n# Invasive Breast Cancer\\n\\n# Table of Contents\\n\\n# Discussion\\n\\n# TARGETED THERAPIES AND ASSOCIATED BIOMARKER TESTING FOR RECURRENT UNRESECTABLE (LOCAL OR REGIONAL) OR STAGE IV (M1) DISEASE\\n\\n# REFERENCES\\n\\n1. Andre F, Ciruelos E, Rubovszky G, et al. Alpelisib for PIK3CA-mutated, hormone receptor-positive advanced breast cancer. N Engl J Med 2019;380:1929-1940.\\n2. Turner NC, Oliveira M, Howell SJ, et al. Capivasertib in hormone receptor–positive advanced breast cancer. N Engl J Med 2023;388:2058-2070.\\n3. Berton D, Banerjee S, Curigliano G, et al. Antitumor activity of dostarlimab in patients with mismatch repair–deficient (dMMR) tumors: a combined analysis of 2 cohorts in the GARNET study. Poster presented at American Society for Clinical Oncology (ASCO), Virtual Meeting, June 4–8, 2021. [Abstract ID: 2564].\\n4. Bidard FC, Kaklamani V, Neven P, et al. Elacestrant (oral selective estrogen receptor degrader) versus standard endocrine '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pages[100].md[:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a71cf7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# NCCN Guidelines Version 4.2025\\n\\n# NCCN Guidelines Index\\n\\n# Invasive Breast Cancer\\n\\n# Table of Contents\\n\\n# Discussion\\n\\n# TARGETED THERAPIES AND ASSOCIATED BIOMARKER TESTING FOR RECURRENT UNRESECTABLE (LOCAL OR REGIONAL) OR STAGE IV (M1) DISEASE\\n\\n# REFERENCES\\n\\n1. Andre F, Ciruelos E, Rubovszky G, et al. Alpelisib for PIK3CA-mutated, hormone receptor-positive advanced breast cancer. N Engl J Med 2019;380:1929-1940.\\n2. Turner NC, Oliveira M, Howell SJ, et al. Capivasertib in hormone receptor–positive advanced breast cancer. N Engl J Med 2023;388:2058-2070.\\n3. Berton D, Banerjee S, Curigliano G, et al. Antitumor activity of dostarlimab in patients with mismatch repair–deficient (dMMR) tumors: a combined analysis of 2 cohorts in the GARNET study. Poster presented at American Society for Clinical Oncology (ASCO), Virtual Meeting, June 4–8, 2021. [Abstract ID: 2564].\\n4. Bidard FC, Kaklamani V, Neven P, et al. Elacestrant (oral selective estrogen receptor degrader) versus standard endocrine t'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for page in results.pages:\n",
    "    if hasattr(page, 'text') and page.text:\n",
    "        page.text = page.text.strip()\n",
    "    if hasattr(page, 'md') and page.md:\n",
    "        page.md = page.md.strip()\n",
    "        \n",
    "results.pages[100].md[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c104d894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.charts for p in results.pages if p.charts != []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21e05895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.tables for p in results.pages if p.tables != []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c82d8e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[ImageItem(name='img_p124_1.png', height=167.0, width=486.0, x=23.5500755, y=19.450134299999966, original_width=486, original_height=167, type=None)],\n",
       " [ImageItem(name='img_p125_1.png', height=167.0, width=486.0, x=23.5500755, y=19.450134299999966, original_width=486, original_height=167, type=None),\n",
       "  ImageItem(name='img_p125_2.png', height=729.0, width=729.0, x=221.5500641, y=176.29901130000002, original_width=729, original_height=729, type=None)]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.images for p in results.pages if p.images != []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16b9d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core.schema import Document\n",
    "# documents = [\n",
    "#     Document(text=page.md)\n",
    "#     for page in results.pages\n",
    "# ]\n",
    "# len(documents)\n",
    "\n",
    "# get page metadata via this convenience method\n",
    "documents = results.get_markdown_documents(split_by_page=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc70a3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# NCCN Clinical Practice Guidelines in Oncology (NCCN Guidelines®)\\n\\n# Breast Cancer\\n\\n# Version 4.2025 — April 17, 2025\\n\\nNCCN.org\\n\\nNCCN recognizes the importance of clinical trials and encourages participation when applicable and available. Trials should be designed to maximize inclusiveness and broad representative enrollment.\\n\\nNCCN Guidelines for Patients® available at www.nccn.org/patients\\n\\nContinue\\n\\nVersion 4.2025, 4/17/25 © 2025 National Comprehensive Cancer Network® (NCCN®), All rights reserved. NCCN Guidelines® and this illustration may not be reproduced in any form without the express written permission of NCCN.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ceef57eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3256"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build paragraph-level TextNodes with provenance metadata\n",
    "from pathlib import Path\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "doc_id = Path(pdf_path).stem\n",
    "doc_title = doc_id  # optionally parse from documents[0].text\n",
    "nodes = []\n",
    "\n",
    "for page_idx, doc in enumerate(documents):  # documents from LlamaParse\n",
    "    page_text = doc.text or \"\"\n",
    "    # simple paragraph split; adjust to your needs\n",
    "    paragraphs = [p.strip() for p in page_text.split(\"\\n\\n\") if p.strip()]\n",
    "    offset = 0\n",
    "    for para_idx, para in enumerate(paragraphs):\n",
    "        start = page_text.find(para, offset)\n",
    "        end = start + len(para) if start >= 0 else None\n",
    "        offset = (end or offset)\n",
    "        node = TextNode(\n",
    "            text=para,\n",
    "            metadata={\n",
    "                \"doc_id\": doc_id,\n",
    "                \"doc_title\": doc_title,\n",
    "                \"file_path\": pdf_path,\n",
    "                \"page_number\": page_idx + 1,\n",
    "                \"paragraph_index\": para_idx,\n",
    "                \"char_start\": start,\n",
    "                \"char_end\": end,\n",
    "            },\n",
    "        )\n",
    "        nodes.append(node)\n",
    "\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f08535f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# NCCN Clinical Practice Guidelines in Oncology (NCCN Guidelines®)\\n\\n# Breast Cancer\\n\\n# Version 4.2025 — April 17, 2025\\n\\nNCCN.org\\n\\nNCCN recognizes the importance of clinical trials and encourages participation when applicable and available. Trials should be designed to maximize inclusiveness and broad representative enrollment.\\n\\nNCCN Guidelines for Patients® available at www.nccn.org/patients\\n\\nContinue\\n\\nVersion 4.2025, 4/17/25 © 2025 National Comprehensive Cancer Network® (NCCN®), All rights reserved. NCCN Guidelines® and this illustration may not be reproduced in any form without the express written permission of NCCN.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c54c1df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_number': 1, 'file_name': '../data/nccn_breast_cancer.pdf'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a3e95c",
   "metadata": {},
   "source": [
    "## 3. GraphRAGExtractor\n",
    "\n",
    "The `GraphRAGExtractor` class is designed to extract triples (subject-relation-object) from text and enrich them by adding descriptions for entities and relationships to their properties using an LLM.\n",
    "\n",
    "This functionality is similar to that of the SimpleLLMPathExtractor, but includes additional enhancements to handle entity, relationship descriptions. For guidance on implementation, you may look at similar existing extractors.\n",
    "\n",
    "Here's a breakdown of its functionality:\n",
    "\n",
    "**Key Components**:\n",
    "\n",
    "- llm: The language model used for extraction.\n",
    "- extract_prompt: A prompt template used to guide the LLM in extracting information.\n",
    "- parse_fn: A function to parse the LLM's output into structured data.\n",
    "- max_paths_per_chunk: Limits the number of triples extracted per text chunk.\n",
    "- num_workers: For parallel processing of multiple text nodes.\n",
    "\n",
    "**Main Methods**:\n",
    "\n",
    "- **call**: The entry point for processing a list of text nodes.\n",
    "- acall: An asynchronous version of call for improved performance.\n",
    "- \\_aextract: The core method that processes each individual node.\n",
    "\n",
    "**Extraction Process**:\n",
    "\n",
    "For each input node (chunk of text):\n",
    "\n",
    "1.  It sends the text to the LLM along with the extraction prompt.\n",
    "2.  The LLM's response is parsed to extract entities, relationships, descriptions for entities and relations.\n",
    "3.  Entities are converted into EntityNode objects. Entity description is stored in metadata\n",
    "4.  Relationships are converted into Relation objects. Relationship description is stored in metadata.\n",
    "5.  These are added to the node's metadata under KG_NODES_KEY and KG_RELATIONS_KEY.\n",
    "\n",
    "NOTE: In the current implementation, we are using only relationship descriptions. In the next implementation, we will utilize entity descriptions during the retrieval stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45baa747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from typing import Any, List, Callable, Optional, Union, Dict\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from llama_index.core.async_utils import run_jobs\n",
    "from llama_index.core.indices.property_graph.utils import (\n",
    "    default_parse_triplets_fn,\n",
    ")\n",
    "from llama_index.core.graph_stores.types import (\n",
    "    EntityNode,\n",
    "    KG_NODES_KEY,\n",
    "    KG_RELATIONS_KEY,\n",
    "    Relation,\n",
    ")\n",
    "from llama_index.core.llms.llm import LLM\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.prompts.default_prompts import (\n",
    "    DEFAULT_KG_TRIPLET_EXTRACT_PROMPT,\n",
    ")\n",
    "from llama_index.core.schema import TransformComponent, BaseNode\n",
    "from llama_index.core.bridge.pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class GraphRAGExtractor(TransformComponent):\n",
    "    \"\"\"Extract triples from text nodes, producing entities/relations with provenance.\n",
    "\n",
    "    Uses an LLM and a simple prompt + output parsing to extract paths (i.e. triples) and entity, relation descriptions from text.\n",
    "\n",
    "    Args:\n",
    "        llm (LLM):\n",
    "            The language model to use.\n",
    "        extract_prompt (Union[str, PromptTemplate]):\n",
    "            The prompt to use for extracting triples.\n",
    "        parse_fn (callable):\n",
    "            A function to parse the output of the language model.\n",
    "        num_workers (int):\n",
    "            The number of workers to use for parallel processing.\n",
    "        max_paths_per_chunk (int):\n",
    "            The maximum number of paths to extract per chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    llm: LLM\n",
    "    extract_prompt: PromptTemplate\n",
    "    parse_fn: Callable\n",
    "    num_workers: int\n",
    "    max_paths_per_chunk: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: Optional[LLM] = None,\n",
    "        extract_prompt: Optional[Union[str, PromptTemplate]] = None,\n",
    "        parse_fn: Callable = default_parse_triplets_fn,\n",
    "        max_paths_per_chunk: int = 10,\n",
    "        num_workers: int = 4,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        from llama_index.core import Settings\n",
    "\n",
    "        if isinstance(extract_prompt, str):\n",
    "            extract_prompt = PromptTemplate(extract_prompt)\n",
    "\n",
    "        super().__init__(\n",
    "            llm=llm or Settings.llm,\n",
    "            extract_prompt=extract_prompt or DEFAULT_KG_TRIPLET_EXTRACT_PROMPT,\n",
    "            parse_fn=parse_fn,\n",
    "            num_workers=num_workers,\n",
    "            max_paths_per_chunk=max_paths_per_chunk,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def class_name(cls) -> str:\n",
    "        return \"GraphExtractor\"\n",
    "\n",
    "    def __call__(\n",
    "        self, nodes: List[BaseNode], show_progress: bool = False, **kwargs: Any\n",
    "    ) -> List[BaseNode]:\n",
    "        \"\"\"Extract triples from nodes.\"\"\"\n",
    "        return asyncio.run(self.acall(nodes, show_progress=show_progress, **kwargs))\n",
    "\n",
    "    async def _aextract(self, node: BaseNode) -> BaseNode:\n",
    "        \"\"\"Extract triples from a node, attaching provenance to relations.\n",
    "\n",
    "        Returns:\n",
    "            BaseNode: The same node with KG nodes/relations added into metadata.\n",
    "        \"\"\"\n",
    "        assert hasattr(node, \"text\")\n",
    "\n",
    "        text = node.get_content(metadata_mode=\"llm\")\n",
    "        try:\n",
    "            llm_response = await self.llm.apredict(\n",
    "                self.extract_prompt,\n",
    "                text=text,\n",
    "                max_knowledge_triplets=self.max_paths_per_chunk,\n",
    "            )\n",
    "            entities, entities_relationship = self.parse_fn(llm_response)\n",
    "        except ValueError:\n",
    "            entities = []\n",
    "            entities_relationship = []\n",
    "\n",
    "        existing_nodes = node.metadata.pop(KG_NODES_KEY, [])\n",
    "        existing_relations = node.metadata.pop(KG_RELATIONS_KEY, [])\n",
    "        entity_metadata = node.metadata.copy()\n",
    "        for entity, entity_type, description in entities:\n",
    "            entity_metadata[\"entity_description\"] = description\n",
    "            entity_node = EntityNode(\n",
    "                name=entity, label=entity_type, properties=entity_metadata\n",
    "            )\n",
    "            existing_nodes.append(entity_node)\n",
    "\n",
    "        relation_metadata_base = node.metadata.copy()\n",
    "        # Baseline provenance from node metadata\n",
    "        base_prov = {\n",
    "            \"source_doc_id\": relation_metadata_base.get(\"doc_id\"),\n",
    "            \"source_doc_title\": relation_metadata_base.get(\"doc_title\"),\n",
    "            \"source_file_path\": relation_metadata_base.get(\"file_path\"),\n",
    "            \"source_page\": relation_metadata_base.get(\"page_number\"),\n",
    "            \"source_paragraph_index\": relation_metadata_base.get(\"paragraph_index\"),\n",
    "            \"char_start\": relation_metadata_base.get(\"char_start\"),\n",
    "            \"char_end\": relation_metadata_base.get(\"char_end\"),\n",
    "            \"extraction_node_id\": getattr(node, \"node_id\", None),\n",
    "        }\n",
    "        # Create a short snippet window\n",
    "        try:\n",
    "            full_text = node.get_content(metadata_mode=\"llm\") or \"\"\n",
    "        except Exception:\n",
    "            full_text = \"\"\n",
    "        snippet = full_text\n",
    "        if isinstance(base_prov.get(\"char_start\"), int) and isinstance(\n",
    "            base_prov.get(\"char_end\"), int\n",
    "        ):\n",
    "            s, e = base_prov[\"char_start\"], base_prov[\"char_end\"]\n",
    "            if 0 <= s < e <= len(full_text):\n",
    "                snippet = full_text[s:e]\n",
    "        snippet = (snippet or full_text)[:500]\n",
    "\n",
    "        for triple in entities_relationship:\n",
    "            subj, obj, rel, description = triple\n",
    "            relation_metadata = relation_metadata_base.copy()\n",
    "            relation_metadata[\"relationship_description\"] = description\n",
    "            triplet_key = f\"{subj}|{rel}|{obj}\"\n",
    "            # Compute provenance id\n",
    "            prov_id_payload = json.dumps(\n",
    "                {**base_prov, \"triplet_key\": triplet_key},\n",
    "                default=str,\n",
    "                ensure_ascii=False,\n",
    "            )\n",
    "            prov_id = hashlib.sha256(prov_id_payload.encode(\"utf-8\")).hexdigest()\n",
    "            relation_metadata.update(\n",
    "                {\n",
    "                    \"triplet_key\": triplet_key,\n",
    "                    \"source_doc_id\": base_prov[\"source_doc_id\"],\n",
    "                    \"source_doc_title\": base_prov[\"source_doc_title\"],\n",
    "                    \"source_file_path\": base_prov[\"source_file_path\"],\n",
    "                    \"source_page\": base_prov[\"source_page\"],\n",
    "                    \"source_paragraph_index\": base_prov[\"source_paragraph_index\"],\n",
    "                    \"char_start\": base_prov[\"char_start\"],\n",
    "                    \"char_end\": base_prov[\"char_end\"],\n",
    "                    \"extraction_node_id\": base_prov[\"extraction_node_id\"],\n",
    "                    \"source_snippet\": snippet,\n",
    "                    \"provenance_id\": prov_id,\n",
    "                }\n",
    "            )\n",
    "            rel_node = Relation(\n",
    "                label=rel,\n",
    "                source_id=subj,\n",
    "                target_id=obj,\n",
    "                properties=relation_metadata,\n",
    "            )\n",
    "\n",
    "            existing_relations.append(rel_node)\n",
    "\n",
    "        node.metadata[KG_NODES_KEY] = existing_nodes\n",
    "        node.metadata[KG_RELATIONS_KEY] = existing_relations\n",
    "        return node\n",
    "\n",
    "    async def acall(\n",
    "        self, nodes: List[BaseNode], show_progress: bool = False, **kwargs: Any\n",
    "    ) -> List[BaseNode]:\n",
    "        \"\"\"Extract triples from nodes async.\"\"\"\n",
    "        jobs = []\n",
    "        for node in nodes:\n",
    "            jobs.append(self._aextract(node))\n",
    "\n",
    "        return await run_jobs(\n",
    "            jobs,\n",
    "            workers=self.num_workers,\n",
    "            show_progress=show_progress,\n",
    "            desc=\"Extracting paths from text\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c02dafe",
   "metadata": {},
   "source": [
    "## 4. GraphRAGStore\n",
    "\n",
    "The GraphRAGStore class is an extension of the Neo4jPropertyGraphStoreclass, designed to implement GraphRAG pipeline. Here's a breakdown of its key components and functions:\n",
    "\n",
    "The class uses community detection algorithms to group related nodes in the graph and then it generates summaries for each community using an LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4beb2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmui/SynologyDrive/research/2025/bioreasoning/.venv/lib/python3.12/site-packages/graspologic/layouts/colors.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import re\n",
    "import networkx as nx\n",
    "from graspologic.partition import hierarchical_leiden\n",
    "from collections import defaultdict\n",
    "\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
    "\n",
    "\n",
    "class GraphRAGStore(Neo4jPropertyGraphStore):\n",
    "    \"\"\"Property-graph store with community building, caching, and provenance support.\n",
    "\n",
    "    Extends the Neo4j store to:\n",
    "    - Build NetworkX graphs and detect communities\n",
    "    - Generate LLM-based community summaries\n",
    "    - Persist and load a cache file with community data and triplet provenance\n",
    "    - Provide helpers to backfill provenance from graph relations\n",
    "    \"\"\"\n",
    "\n",
    "    community_summary = {}\n",
    "    entity_info = None\n",
    "    max_cluster_size = 5\n",
    "    # Added fields to capture full, reproducible graph community state\n",
    "    cluster_assignments: Dict[str, int] | None = None  # node -> cluster id\n",
    "    community_info: Dict[int, List[str]] | None = (\n",
    "        None  # cluster id -> relationship detail strings\n",
    "    )\n",
    "    algorithm_metadata: Dict[str, Any] | None = None\n",
    "    # Provenance map: triplet_key -> list of provenance dicts\n",
    "    triplet_provenance: Dict[str, List[Dict[str, Any]]] | None = None\n",
    "    \n",
    "    @property\n",
    "    def supports_vector_queries(self) -> bool:\n",
    "        \"\"\"Return whether this store supports vector queries.\"\"\"\n",
    "        return False\n",
    "\n",
    "    def generate_community_summary(self, text):\n",
    "        \"\"\"Generate summary for a given text using an LLM.\"\"\"\n",
    "        messages = [\n",
    "            ChatMessage(\n",
    "                role=\"system\",\n",
    "                content=(\n",
    "                    \"You are provided with a set of relationships from a knowledge graph, each represented as \"\n",
    "                    \"entity1->entity2->relation->relationship_description. Your task is to create a summary of these \"\n",
    "                    \"relationships. The summary should include the names of the entities involved and a concise synthesis \"\n",
    "                    \"of the relationship descriptions. The goal is to capture the most critical and relevant details that \"\n",
    "                    \"highlight the nature and significance of each relationship. Ensure that the summary is coherent and \"\n",
    "                    \"integrates the information in a way that emphasizes the key aspects of the relationships.\"\n",
    "                ),\n",
    "            ),\n",
    "            ChatMessage(role=\"user\", content=text),\n",
    "        ]\n",
    "        response = OpenAI(model=\"gpt-4.1-mini\").chat(messages)\n",
    "        clean_response = re.sub(r\"^assistant:\\s*\", \"\", str(response)).strip()\n",
    "        return clean_response\n",
    "\n",
    "    def build_communities(self):\n",
    "        \"\"\"Builds communities from the graph and summarizes them.\"\"\"\n",
    "        nx_graph = self._create_nx_graph()\n",
    "        # Try using graspologic hierarchical_leiden; fallback to connected components\n",
    "        try:\n",
    "            from graspologic.partition import hierarchical_leiden  # type: ignore\n",
    "\n",
    "            community_hierarchical_clusters = hierarchical_leiden(\n",
    "                nx_graph, max_cluster_size=self.max_cluster_size\n",
    "            )\n",
    "            algorithm_used = \"hierarchical_leiden\"\n",
    "            library_used = \"graspologic.partition\"\n",
    "        except Exception:\n",
    "            # Fallback: simple connected components as clusters\n",
    "            components = list(nx.connected_components(nx_graph))\n",
    "\n",
    "            class _ClusterItem:\n",
    "                def __init__(self, node, cluster):\n",
    "                    self.node = node\n",
    "                    self.cluster = cluster\n",
    "\n",
    "            items = []\n",
    "            for idx, comp in enumerate(components):\n",
    "                for node in comp:\n",
    "                    items.append(_ClusterItem(node, idx))\n",
    "            community_hierarchical_clusters = items\n",
    "            algorithm_used = \"connected_components\"\n",
    "            library_used = \"networkx\"\n",
    "        self.entity_info, community_info = self._collect_community_info(\n",
    "            nx_graph, community_hierarchical_clusters\n",
    "        )\n",
    "        # Persist additional state for reproducibility\n",
    "        # Flatten cluster assignments\n",
    "        try:\n",
    "            assignments: Dict[str, int] = {}\n",
    "            for item in community_hierarchical_clusters:\n",
    "                node = getattr(item, \"node\", None)\n",
    "                cluster_id = getattr(item, \"cluster\", None)\n",
    "                if node is not None and cluster_id is not None:\n",
    "                    assignments[str(node)] = int(cluster_id)\n",
    "            self.cluster_assignments = assignments\n",
    "        except Exception:\n",
    "            self.cluster_assignments = None\n",
    "\n",
    "        self.community_info = community_info\n",
    "        self.algorithm_metadata = {\n",
    "            \"algorithm\": algorithm_used,\n",
    "            \"library\": library_used,\n",
    "            \"parameters\": {\"max_cluster_size\": self.max_cluster_size},\n",
    "            \"nx_graph_nodes\": nx_graph.number_of_nodes(),\n",
    "            \"nx_graph_edges\": nx_graph.number_of_edges(),\n",
    "        }\n",
    "\n",
    "        self._summarize_communities(community_info)\n",
    "\n",
    "    def _create_nx_graph(self):\n",
    "        \"\"\"Converts internal graph representation to NetworkX graph.\n",
    "\n",
    "        Returns:\n",
    "            nx.Graph: Nodes are entity names. Edges carry 'relationship', 'description', and 'triplet_key'.\n",
    "        \"\"\"\n",
    "        nx_graph = nx.Graph()\n",
    "        triplets = self.get_triplets()\n",
    "        for entity1, relation, entity2 in triplets:\n",
    "            nx_graph.add_node(entity1.name)\n",
    "            nx_graph.add_node(entity2.name)\n",
    "            triplet_key = f\"{relation.source_id}|{relation.label}|{relation.target_id}\"\n",
    "            nx_graph.add_edge(\n",
    "                relation.source_id,\n",
    "                relation.target_id,\n",
    "                relationship=relation.label,\n",
    "                description=relation.properties[\"relationship_description\"],\n",
    "                triplet_key=triplet_key,\n",
    "            )\n",
    "        return nx_graph\n",
    "\n",
    "    def _collect_community_info(self, nx_graph, clusters):\n",
    "        \"\"\"Aggregate per-community relationship details.\n",
    "\n",
    "        Collect information for each node based on its community assignment,\n",
    "        allowing entities to belong to multiple clusters.\n",
    "\n",
    "        Returns:\n",
    "            tuple(dict, dict): (entity_info, community_info)\n",
    "                - entity_info maps node -> list[cluster_id]\n",
    "                - community_info maps cluster_id -> list[{detail, triplet_key}]\n",
    "        \"\"\"\n",
    "        entity_info = defaultdict(set)\n",
    "        community_info = defaultdict(list)\n",
    "\n",
    "        for item in clusters:\n",
    "            node = item.node\n",
    "            cluster_id = item.cluster\n",
    "\n",
    "            # Update entity_info\n",
    "            entity_info[node].add(cluster_id)\n",
    "\n",
    "            for neighbor in nx_graph.neighbors(node):\n",
    "                edge_data = nx_graph.get_edge_data(node, neighbor)\n",
    "                if edge_data:\n",
    "                    detail = f\"{node} -> {neighbor} -> {edge_data['relationship']} -> {edge_data['description']}\"\n",
    "                    community_info[cluster_id].append(\n",
    "                        {\n",
    "                            \"detail\": detail,\n",
    "                            \"triplet_key\": edge_data.get(\"triplet_key\"),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        # Convert sets to lists for easier serialization if needed\n",
    "        entity_info = {k: list(v) for k, v in entity_info.items()}\n",
    "\n",
    "        return dict(entity_info), dict(community_info)\n",
    "\n",
    "    def _summarize_communities(self, community_info):\n",
    "        \"\"\"Generate and store LLM summaries for each community.\"\"\"\n",
    "        for community_id, details in community_info.items():\n",
    "            # Support both legacy list[str] and new list[dict]\n",
    "            if details and isinstance(details[0], dict):\n",
    "                text_lines = [d.get(\"detail\", \"\") for d in details]\n",
    "            else:\n",
    "                text_lines = [str(d) for d in details]\n",
    "            details_text = \"\\n\".join(text_lines) + \".\"\n",
    "            self.community_summary[community_id] = self.generate_community_summary(\n",
    "                details_text\n",
    "            )\n",
    "\n",
    "    def get_community_summaries(self):\n",
    "        \"\"\"Returns the community summaries, building them if not already done.\"\"\"\n",
    "        if not self.community_summary:\n",
    "            self.build_communities()\n",
    "        return self.community_summary\n",
    "\n",
    "    def _compute_graph_signature(self) -> str:\n",
    "        \"\"\"Compute a stable signature of the current graph triplets for cache validation.\"\"\"\n",
    "        triplets = self.get_triplets()\n",
    "        items = []\n",
    "        for entity1, relation, entity2 in triplets:\n",
    "            relation_desc = \"\"\n",
    "            try:\n",
    "                relation_desc = relation.properties.get(\"relationship_description\", \"\")\n",
    "            except Exception:\n",
    "                relation_desc = \"\"\n",
    "            items.append((entity1.name, relation.label, entity2.name, relation_desc))\n",
    "        # Ensure deterministic ordering\n",
    "        items.sort()\n",
    "        payload = json.dumps(items, ensure_ascii=False, separators=(\",\", \":\")).encode(\n",
    "            \"utf-8\"\n",
    "        )\n",
    "        return hashlib.sha256(payload).hexdigest()\n",
    "\n",
    "    def save_communities(self, filepath: str) -> None:\n",
    "        \"\"\"Persist full community/graph-derived state to a JSON file.\n",
    "\n",
    "        Notes:\n",
    "        - Nodes and relationships (and their properties) live in Neo4j via the base store.\n",
    "        - This file captures the rest needed to reproduce community-based answering without recomputation.\n",
    "        \"\"\"\n",
    "        # Ensure destination directory exists\n",
    "        directory = os.path.dirname(filepath)\n",
    "        if directory:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        data = {\n",
    "            \"graph_signature\": self._compute_graph_signature(),\n",
    "            \"max_cluster_size\": self.max_cluster_size,\n",
    "            \"entity_info\": self.entity_info or {},\n",
    "            \"community_summary\": self.community_summary or {},\n",
    "            # Extended state for reproducibility\n",
    "            \"cluster_assignments\": self.cluster_assignments or {},\n",
    "            \"community_info\": self.community_info or {},\n",
    "            \"triplet_provenance\": self.triplet_provenance or {},\n",
    "            \"algorithm_metadata\": self.algorithm_metadata\n",
    "            or {\n",
    "                \"algorithm\": \"hierarchical_leiden\",\n",
    "                \"parameters\": {\"max_cluster_size\": self.max_cluster_size},\n",
    "            },\n",
    "        }\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def load_communities(self, filepath: str, validate_signature: bool = True) -> bool:\n",
    "        \"\"\"Load community data from a JSON file. Returns True if loaded successfully.\n",
    "\n",
    "        If validate_signature is True, the on-disk signature must match the current\n",
    "        graph signature (derived from current triplets); otherwise, the cache is ignored.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "        except json.JSONDecodeError:\n",
    "            return False\n",
    "\n",
    "        if validate_signature:\n",
    "            try:\n",
    "                current_sig = self._compute_graph_signature()\n",
    "            except Exception:\n",
    "                current_sig = None\n",
    "            if not current_sig or data.get(\"graph_signature\") != current_sig:\n",
    "                return False\n",
    "\n",
    "        self.entity_info = data.get(\"entity_info\") or {}\n",
    "        # Convert community_summary string keys to integers for consistency\n",
    "        raw_cs = data.get(\"community_summary\") or {}\n",
    "        norm_cs = {}\n",
    "        for cid, summary in raw_cs.items():\n",
    "            try:\n",
    "                int_cid = int(cid)\n",
    "            except (ValueError, TypeError):\n",
    "                int_cid = cid  # Keep original if conversion fails\n",
    "            norm_cs[int_cid] = summary\n",
    "        self.community_summary = norm_cs\n",
    "        # Populate extended state if present\n",
    "        self.cluster_assignments = data.get(\"cluster_assignments\") or {}\n",
    "        # Normalize community_info: ensure list of dicts with 'detail' and optional 'triplet_key'\n",
    "        # Also convert string keys to integers for consistency\n",
    "        raw_ci = data.get(\"community_info\") or {}\n",
    "        norm_ci: Dict[int, List[Dict[str, Any]]] = {}\n",
    "        for cid, items in raw_ci.items():\n",
    "            # Convert string keys to integers\n",
    "            try:\n",
    "                int_cid = int(cid)\n",
    "            except (ValueError, TypeError):\n",
    "                int_cid = cid  # Keep original if conversion fails\n",
    "            \n",
    "            new_items: List[Dict[str, Any]] = []\n",
    "            for it in items:\n",
    "                if isinstance(it, dict):\n",
    "                    # already new format\n",
    "                    new_items.append(\n",
    "                        {\n",
    "                            \"detail\": it.get(\"detail\", \"\"),\n",
    "                            \"triplet_key\": it.get(\"triplet_key\"),\n",
    "                        }\n",
    "                    )\n",
    "                else:\n",
    "                    new_items.append({\"detail\": str(it), \"triplet_key\": None})\n",
    "            norm_ci[int_cid] = new_items\n",
    "        self.community_info = norm_ci\n",
    "        self.triplet_provenance = data.get(\"triplet_provenance\") or {}\n",
    "        self.algorithm_metadata = data.get(\"algorithm_metadata\") or None\n",
    "        return True\n",
    "\n",
    "    def has_graph_data(self) -> bool:\n",
    "        \"\"\"Return True if the backing graph store has any triplets.\"\"\"\n",
    "        try:\n",
    "            triplets = self.get_triplets()\n",
    "        except Exception:\n",
    "            return False\n",
    "        return bool(triplets)\n",
    "\n",
    "    def ensure_communities(\n",
    "        self,\n",
    "        persist_path: str | None = None,\n",
    "        validate_signature: bool = True,\n",
    "        prefer_cache_when_graph_empty: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"Ensure communities are available, optionally using a persisted cache.\n",
    "\n",
    "        If persist_path is provided, attempt to load; if that fails, build and then save.\n",
    "        \"\"\"\n",
    "        if persist_path:\n",
    "            loaded = False\n",
    "            # If graph is empty and we prefer cache, load without strict validation\n",
    "            if prefer_cache_when_graph_empty and not self.has_graph_data():\n",
    "                loaded = self.load_communities(persist_path, validate_signature=False)\n",
    "            else:\n",
    "                loaded = self.load_communities(\n",
    "                    persist_path, validate_signature=validate_signature\n",
    "                )\n",
    "            if loaded:\n",
    "                # Auto-backfill provenance if missing but graph has data\n",
    "                if (\n",
    "                    (not getattr(self, \"triplet_provenance\", None) or not self.triplet_provenance)\n",
    "                    and self.has_graph_data()\n",
    "                ):\n",
    "                    self.build_triplet_provenance_from_graph()\n",
    "                    try:\n",
    "                        self.save_communities(persist_path)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                return\n",
    "        self.build_communities()\n",
    "        if persist_path:\n",
    "            self.save_communities(persist_path)\n",
    "\n",
    "    def build_triplet_provenance_from_graph(self) -> None:\n",
    "        \"\"\"Backfill triplet_provenance by reading relation properties from the graph store.\n",
    "\n",
    "        Iterates current triplets and collects provenance-relevant fields from each relation's properties.\n",
    "        Intended for use when a cache file lacks triplet_provenance.\n",
    "        \"\"\"\n",
    "        provenance: Dict[str, List[Dict[str, Any]]] = {}\n",
    "        try:\n",
    "            triplets = self.get_triplets()\n",
    "        except Exception:\n",
    "            triplets = []\n",
    "        for e1, relation, e2 in triplets:\n",
    "            props_r = getattr(relation, \"properties\", {}) or {}\n",
    "            props_e1 = getattr(e1, \"properties\", {}) or {}\n",
    "            props_e2 = getattr(e2, \"properties\", {}) or {}\n",
    "            triplet_key = (\n",
    "                props_r.get(\"triplet_key\")\n",
    "                or f\"{relation.source_id}|{relation.label}|{relation.target_id}\"\n",
    "            )\n",
    "            # Prefer relation properties; fallback to entity properties\n",
    "            def pick(key: str):\n",
    "                return (\n",
    "                    props_r.get(key)\n",
    "                    or props_e1.get(key)\n",
    "                    or props_e2.get(key)\n",
    "                )\n",
    "\n",
    "            prov = {\n",
    "                \"source_doc_id\": pick(\"doc_id\") or pick(\"source_doc_id\"),\n",
    "                \"source_doc_title\": pick(\"doc_title\")\n",
    "                or pick(\"source_doc_title\"),\n",
    "                \"source_file_path\": pick(\"file_path\")\n",
    "                or pick(\"source_file_path\"),\n",
    "                \"source_page\": pick(\"page_number\") or pick(\"source_page\"),\n",
    "                \"source_paragraph_index\": pick(\"paragraph_index\")\n",
    "                or pick(\"source_paragraph_index\"),\n",
    "                \"char_start\": pick(\"char_start\"),\n",
    "                \"char_end\": pick(\"char_end\"),\n",
    "                \"source_snippet\": make_contextual_snippet(\n",
    "                    props_r.get(\"source_snippet\")\n",
    "                    or props_r.get(\"relationship_description\")\n",
    "                    or props_e1.get(\"entity_description\")\n",
    "                    or props_e2.get(\"entity_description\")\n",
    "                    or \"\",\n",
    "                    \"\",\n",
    "                    max_length=500\n",
    "                ),\n",
    "                \"extraction_node_id\": props_r.get(\"extraction_node_id\"),\n",
    "                \"provenance_id\": props_r.get(\"provenance_id\"),\n",
    "            }\n",
    "            if not prov[\"provenance_id\"]:\n",
    "                payload = json.dumps(\n",
    "                    {**prov, \"triplet_key\": triplet_key},\n",
    "                    default=str,\n",
    "                    ensure_ascii=False,\n",
    "                )\n",
    "                prov[\"provenance_id\"] = hashlib.sha256(\n",
    "                    payload.encode(\"utf-8\")\n",
    "                ).hexdigest()\n",
    "            provenance.setdefault(triplet_key, [])\n",
    "            # Deduplicate by provenance_id\n",
    "            if all(\n",
    "                p.get(\"provenance_id\") != prov[\"provenance_id\"]\n",
    "                for p in provenance[triplet_key]\n",
    "            ):\n",
    "                provenance[triplet_key].append(prov)\n",
    "        self.triplet_provenance = provenance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2b410f",
   "metadata": {},
   "source": [
    "## 5. GraphRAGQueryEngine\n",
    "\n",
    "The GraphRAGQueryEngine class is a custom query engine designed to process queries using the GraphRAG approach. It leverages the community summaries generated by the GraphRAGStore to answer user queries. Here's a breakdown of its functionality:\n",
    "\n",
    "Main Components:\n",
    "\n",
    "`graph_store`: An instance of GraphRAGStore, which contains the community summaries. llm: A Language Model (LLM) used for generating and aggregating answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39991189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import CustomQueryEngine\n",
    "from llama_index.core.llms import LLM\n",
    "from llama_index.core import PropertyGraphIndex\n",
    "\n",
    "class GraphRAGQueryEngine(CustomQueryEngine):\n",
    "    \"\"\"Query engine over community summaries and provenance-bearing triplets.\n",
    "\n",
    "    Strategy:\n",
    "    - Resolve entities via retriever or fallback vocabulary scan\n",
    "    - Rank communities by keyword overlap, then rank triplets in those communities\n",
    "    - Build cited evidence bullets and instruct LLM to answer with [n] markers\n",
    "    - Append a citations section to the response\n",
    "    \"\"\"\n",
    "\n",
    "    graph_store: GraphRAGStore\n",
    "    index: PropertyGraphIndex\n",
    "    llm: LLM\n",
    "    similarity_top_k: int = 20\n",
    "    max_summaries_to_use: int = 6\n",
    "    max_triplets_to_use: int = 20\n",
    "    # capture last computed citations for programmatic consumers\n",
    "    last_citations: List[Dict[str, Any]] = []\n",
    "\n",
    "    def custom_query(self, query_str: str) -> str:\n",
    "        \"\"\"Answer a query using community summaries and triplet provenance, with citations.\"\"\"\n",
    "\n",
    "        entities = self.get_entities(query_str, self.similarity_top_k)\n",
    "\n",
    "        community_ids = self.retrieve_entity_communities(\n",
    "            self.graph_store.entity_info, entities\n",
    "        )\n",
    "        community_summaries = self.graph_store.get_community_summaries()\n",
    "\n",
    "        # Fallbacks when no entities/communities were resolved via retrieval\n",
    "        if not community_ids:\n",
    "            # If we have any communities, consider all of them\n",
    "            community_ids = list(community_summaries.keys())\n",
    "\n",
    "        # Normalize IDs to strings to match JSON keys\n",
    "        community_ids = [str(cid) for cid in community_ids]\n",
    "\n",
    "        # Rank summaries by simple keyword overlap with the query and select top-k\n",
    "        ranked = self._rank_communities_by_query_overlap(\n",
    "            community_summaries, query_str, community_ids\n",
    "        )\n",
    "        chosen_ids = [str(cid) for cid, _ in ranked[: self.max_summaries_to_use]]\n",
    "\n",
    "        # If ranking produced nothing (e.g., type mismatch), default to first K summaries\n",
    "        if not chosen_ids and community_summaries:\n",
    "            chosen_ids = list(community_summaries.keys())[: self.max_summaries_to_use]\n",
    "\n",
    "        # Build a candidate triplet set and collect detail-only fallbacks\n",
    "        candidate_triplets: List[Tuple[str, str]] = []  # (triplet_key, detail)\n",
    "        detail_only_blocks: List[str] = []\n",
    "        if isinstance(self.graph_store.community_info, dict):\n",
    "            for cid in chosen_ids:\n",
    "                items = self.graph_store.community_info.get(cid, [])\n",
    "                for it in items:\n",
    "                    if isinstance(it, dict):\n",
    "                        detail = it.get(\"detail\", \"\")\n",
    "                        tk = it.get(\"triplet_key\")\n",
    "                        if tk:\n",
    "                            candidate_triplets.append((tk, detail))\n",
    "                        elif detail:\n",
    "                            detail_only_blocks.append(f\"- {detail}\")\n",
    "                    else:\n",
    "                        detail_only_blocks.append(f\"- {str(it)}\")\n",
    "\n",
    "        # Rank triplets by overlap and select top M\n",
    "        ranked_triplets = self._rank_triplets_by_query_overlap(\n",
    "            candidate_triplets, query_str\n",
    "        )\n",
    "        chosen_triplets = ranked_triplets[: self.max_triplets_to_use]\n",
    "\n",
    "        # Prepare context blocks with citations\n",
    "        context_blocks, citations = self._prepare_cited_triplets(chosen_triplets)\n",
    "\n",
    "        # Generate answers with fallbacks\n",
    "        community_answers: List[str] = []\n",
    "        if context_blocks:\n",
    "            community_answers = [\n",
    "                self.generate_answer_from_cited_context(\n",
    "                    \"\\n\".join(context_blocks), query_str\n",
    "                )\n",
    "            ]\n",
    "        elif detail_only_blocks:\n",
    "            community_answers = [\n",
    "                self.generate_answer_from_cited_context(\n",
    "                    \"\\n\".join(detail_only_blocks), query_str\n",
    "                )\n",
    "            ]\n",
    "            citations = []\n",
    "        else:\n",
    "            top_summary_ids = chosen_ids[: self.max_summaries_to_use]\n",
    "            community_answers = [\n",
    "                self.generate_answer_from_summary(community_summaries[cid], query_str)\n",
    "                for cid in top_summary_ids\n",
    "                if cid in community_summaries\n",
    "            ]\n",
    "            citations = []\n",
    "\n",
    "        final_answer = self.aggregate_answers(community_answers)\n",
    "        # # Append citations section for human readability\n",
    "        # if \"citations\" in locals() and citations:\n",
    "        #     lines = [\"\\nCitations:\"]\n",
    "        #     for c in citations:\n",
    "        #         title = c.get(\"title\") or c.get(\"doc_id\") or \"Source\"\n",
    "        #         page = c.get(\"page\")\n",
    "        #         para = c.get(\"paragraph\")\n",
    "        #         snippet = (c.get(\"snippet\") or \"\").strip()\n",
    "        #         loc = []\n",
    "        #         if page is not None:\n",
    "        #             loc.append(f\"p. {page}\")\n",
    "        #         if para is not None:\n",
    "        #             loc.append(f\"¶ {para}\")\n",
    "        #         loc_str = f\" ({', '.join(loc)})\" if loc else \"\"\n",
    "        #         lines.append(f\"[{c['id']}] {title}{loc_str}: {snippet}\")\n",
    "        #     final_answer = final_answer.rstrip() + \"\\n\" + \"\\n\".join(lines)\n",
    "        # Save for programmatic consumers\n",
    "        self.last_citations = citations if \"citations\" in locals() else []\n",
    "        return final_answer\n",
    "\n",
    "    def get_entities(self, query_str, similarity_top_k):\n",
    "        \"\"\"Resolve entities relevant to the query.\n",
    "\n",
    "        Strategy:\n",
    "        1) Try retriever-based extraction from indexed path triples.\n",
    "        2) If nothing found, fallback to vocabulary scanning over graph entities.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            nodes_retrieved = self.index.as_retriever(\n",
    "                similarity_top_k=similarity_top_k\n",
    "            ).retrieve(query_str)\n",
    "        except Exception:\n",
    "            nodes_retrieved = []\n",
    "\n",
    "        entities = set()\n",
    "        pattern = r\"^(\\w+(?:\\s+\\w+)*)\\s*->\\s*([a-zA-Z\\s]+?)\\s*->\\s*(\\w+(?:\\s+\\w+)*)$\"\n",
    "\n",
    "        for node in nodes_retrieved:\n",
    "            matches = re.findall(pattern, node.text, re.MULTILINE | re.IGNORECASE)\n",
    "\n",
    "            for match in matches:\n",
    "                subject = match[0]\n",
    "                obj = match[2]\n",
    "                entities.add(subject)\n",
    "                entities.add(obj)\n",
    "\n",
    "        if entities:\n",
    "            return list(entities)\n",
    "\n",
    "        # Fallback: scan graph vocabulary (entity names) and select those mentioned in query\n",
    "        try:\n",
    "            triplets = self.graph_store.get_triplets()\n",
    "        except Exception:\n",
    "            triplets = []\n",
    "\n",
    "        vocab = set()\n",
    "        for e1, _, e2 in triplets:\n",
    "            try:\n",
    "                vocab.add(str(e1.name))\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                vocab.add(str(e2.name))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        q_lower = query_str.lower()\n",
    "        fallback_entities = [name for name in vocab if name.lower() in q_lower]\n",
    "\n",
    "        return fallback_entities\n",
    "\n",
    "    def _rank_communities_by_query_overlap(\n",
    "        self,\n",
    "        community_summaries: Dict[str, str],\n",
    "        query_str: str,\n",
    "        candidate_ids: List[str],\n",
    "    ) -> List[tuple[str, int]]:\n",
    "        \"\"\"Rank community summaries by simple keyword overlap with the query.\"\"\"\n",
    "        query_terms = {t.lower() for t in re.findall(r\"[a-zA-Z0-9+-]{3,}\", query_str)}\n",
    "        scored: List[tuple[str, int]] = []\n",
    "        for cid in candidate_ids:\n",
    "            summ = community_summaries.get(cid) or \"\"\n",
    "            text_terms = {t.lower() for t in re.findall(r\"[a-zA-Z0-9+-]{3,}\", summ)}\n",
    "            score = len(query_terms & text_terms)\n",
    "            scored.append((cid, score))\n",
    "        scored.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scored\n",
    "\n",
    "    def _rank_triplets_by_query_overlap(\n",
    "        self, triplets: List[Tuple[str, str]], query_str: str\n",
    "    ) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Rank triplets by overlap between query terms and edge 'detail' text.\"\"\"\n",
    "        query_terms = {t.lower() for t in re.findall(r\"[a-zA-Z0-9+-]{3,}\", query_str)}\n",
    "        scored: List[Tuple[int, Tuple[str, str]]] = []\n",
    "        for tk, detail in triplets:\n",
    "            terms = {t.lower() for t in re.findall(r\"[a-zA-Z0-9+-]{3,}\", detail)}\n",
    "            scored.append((len(query_terms & terms), (tk, detail)))\n",
    "        scored.sort(key=lambda x: x[0], reverse=True)\n",
    "        return [item for _, item in scored]\n",
    "\n",
    "    def _prepare_cited_triplets(\n",
    "        self, triplets: List[Tuple[str, str]]\n",
    "    ) -> Tuple[List[str], List[Dict[str, Any]]]:\n",
    "        \"\"\"Build bullet points with [n] and collect citations metadata.\n",
    "\n",
    "        Returns:\n",
    "            tuple(list[str], list[dict]): (context bullets, citations list)\n",
    "        \"\"\"\n",
    "        citations: List[Dict[str, Any]] = []\n",
    "        lines: List[str] = []\n",
    "        seen_prov = set()\n",
    "        for idx, (triplet_key, detail) in enumerate(triplets, start=1):\n",
    "            prov_list = []\n",
    "            if getattr(self.graph_store, \"triplet_provenance\", None):\n",
    "                prov_list = self.graph_store.triplet_provenance.get(triplet_key, [])\n",
    "            # choose one best provenance (first available)\n",
    "            prov = prov_list[0] if prov_list else {}\n",
    "            citation_id = idx\n",
    "            lines.append(f\"- {detail} [{citation_id}]\")\n",
    "            sig = (\n",
    "                prov.get(\"provenance_id\")\n",
    "                or f\"{triplet_key}|{prov.get('source_doc_id')}|{prov.get('source_page')}|{prov.get('source_paragraph_index')}\"\n",
    "            )\n",
    "            if sig in seen_prov:\n",
    "                continue\n",
    "            seen_prov.add(sig)\n",
    "            citations.append(\n",
    "                {\n",
    "                    \"id\": citation_id,\n",
    "                    \"triplet_key\": triplet_key,\n",
    "                    \"doc_id\": prov.get(\"source_doc_id\"),\n",
    "                    \"title\": prov.get(\"source_doc_title\"),\n",
    "                    \"page\": prov.get(\"source_page\"),\n",
    "                    \"paragraph\": prov.get(\"source_paragraph_index\"),\n",
    "                    \"file_path\": prov.get(\"source_file_path\"),\n",
    "                    \"snippet\": prov.get(\"source_snippet\"),\n",
    "                    \"provenance_id\": prov.get(\"provenance_id\"),\n",
    "                }\n",
    "            )\n",
    "        return lines, citations\n",
    "\n",
    "    def retrieve_entity_communities(self, entity_info, entities):\n",
    "        \"\"\"\n",
    "        Retrieve cluster information for given entities, allowing for multiple clusters per entity.\n",
    "\n",
    "        Args:\n",
    "        entity_info (dict): Dictionary mapping entities to their cluster IDs (list).\n",
    "        entities (list): List of entity names to retrieve information for.\n",
    "\n",
    "        Returns:\n",
    "        List of community or cluster IDs to which an entity belongs.\n",
    "        \"\"\"\n",
    "        community_ids = []\n",
    "\n",
    "        for entity in entities:\n",
    "            if entity in entity_info:\n",
    "                community_ids.extend(entity_info[entity])\n",
    "\n",
    "        return list(set(community_ids))\n",
    "\n",
    "    def generate_answer_from_summary(self, community_summary: str, query: str) -> str:\n",
    "        \"\"\"(Legacy) Answer from a single summary.\"\"\"\n",
    "        system_msg = (\n",
    "            \"You are an oncology research assistant. Answer the user's question using only the provided context. \"\n",
    "            \"Be specific and actionable. If the context is insufficient, say so briefly.\"\n",
    "        )\n",
    "        user_msg = f\"Context:\\n{community_summary}\\n\\nQuestion: {query}\\n\\nAnswer strictly from the context above.\"\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=system_msg),\n",
    "            ChatMessage(role=\"user\", content=user_msg),\n",
    "        ]\n",
    "        response = self.llm.chat(messages)\n",
    "        cleaned_response = re.sub(r\"^assistant:\\s*\", \"\", str(response)).strip()\n",
    "        return cleaned_response\n",
    "\n",
    "    def generate_answer_from_cited_context(self, cited_context: str, query: str) -> str:\n",
    "        \"\"\"Answer using cited triplets context; require inline [n] markers in the response.\"\"\"\n",
    "        system_msg = \"You are an oncology research assistant. Use only the provided bullet points; include inline citation markers [n] that refer to those bullets.\"\n",
    "        user_msg = f\"Evidence bullets (with citations):\\n{cited_context}\\n\\nQuestion: {query}\\n\\nWrite a concise answer that includes inline [n] markers referencing the relevant bullets.\"\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=system_msg),\n",
    "            ChatMessage(role=\"user\", content=user_msg),\n",
    "        ]\n",
    "        response = self.llm.chat(messages)\n",
    "        cleaned_response = re.sub(r\"^assistant:\\s*\", \"\", str(response)).strip()\n",
    "        return cleaned_response\n",
    "\n",
    "    def aggregate_answers(self, community_answers: List[str]) -> str:\n",
    "        \"\"\"Aggregate individual community answers into a final, coherent response.\"\"\"\n",
    "        if not community_answers:\n",
    "            return \"No relevant knowledge found in cached community summaries.\"\n",
    "        system_msg = (\n",
    "            \"You combine multiple short medical answers into one concise response. \"\n",
    "            \"Deduplicate, resolve conflicts conservatively, and keep it specific.\"\n",
    "        )\n",
    "        answers_bulleted = \"\\n- \" + \"\\n- \".join(\n",
    "            a.strip() for a in community_answers if a.strip()\n",
    "        )\n",
    "        user_msg = f\"Combine these into one coherent answer:\\n{answers_bulleted}\"\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=system_msg),\n",
    "            ChatMessage(role=\"user\", content=user_msg),\n",
    "        ]\n",
    "        final_response = self.llm.chat(messages)\n",
    "        cleaned_final_response = re.sub(\n",
    "            r\"^assistant:\\s*\", \"\", str(final_response)\n",
    "        ).strip()\n",
    "        return cleaned_final_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed3f5c1",
   "metadata": {},
   "source": [
    "## 6. Build End to End GraphRAG Pipeline\n",
    "\n",
    "Now that we have defined all the necessary components, let’s construct the GraphRAG pipeline:\n",
    "\n",
    "1. Create nodes/chunks from the text.\n",
    "2. Build a PropertyGraphIndex using GraphRAGExtractor and GraphRAGStore.\n",
    "3. Construct communities and generate a summary for each community using the graph built above.\n",
    "4. Create a GraphRAGQueryEngine and begin querying.\n",
    "\n",
    "### Create nodes/ chunks from the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f40f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from llama_index.core.schema import TextNode\n",
    "\n",
    "# nodes = [TextNode(text=document.text) for document in documents]\n",
    "# len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71b8a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# splitter = SentenceSplitter(\n",
    "#     chunk_size=1024,\n",
    "#     chunk_overlap=20,\n",
    "# )\n",
    "# nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f06c06b",
   "metadata": {},
   "source": [
    "### Build ProperGraphIndex using GraphRAGExtractor and GraphRAGStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85e0ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "KG_TRIPLET_EXTRACT_TMPL = \"\"\"\n",
    "-Goal-\n",
    "Given a text document, identify all entities and their entity types from the text and all relationships among the identified entities.\n",
    "Given the text, extract up to {max_knowledge_triplets} entity-relation triplets.\n",
    "\n",
    "-Steps-\n",
    "1. Identify all entities. For each identified entity, extract the following information:\n",
    "- entity_name: Name of the entity, capitalized\n",
    "- entity_type: Type of the entity\n",
    "- entity_description: Comprehensive description of the entity's attributes and activities\n",
    "\n",
    "2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\n",
    "For each pair of related entities, extract the following information:\n",
    "- source_entity: name of the source entity, as identified in step 1\n",
    "- target_entity: name of the target entity, as identified in step 1\n",
    "- relation: relationship between source_entity and target_entity\n",
    "- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n",
    "\n",
    "3. Output Formatting:\n",
    "- Return the result in valid JSON format with two keys: 'entities' (list of entity objects) and 'relationships' (list of relationship objects).\n",
    "- Exclude any text outside the JSON structure (e.g., no explanations or comments).\n",
    "- If no entities or relationships are identified, return empty lists: { \"entities\": [], \"relationships\": [] }.\n",
    "\n",
    "-An Output Example-\n",
    "{\n",
    "  \"entities\": [\n",
    "    {\n",
    "      \"entity_name\": \"Albert Einstein\",\n",
    "      \"entity_type\": \"Person\",\n",
    "      \"entity_description\": \"Albert Einstein was a theoretical physicist who developed the theory of relativity and made significant contributions to physics.\"\n",
    "    },\n",
    "    {\n",
    "      \"entity_name\": \"Theory of Relativity\",\n",
    "      \"entity_type\": \"Scientific Theory\",\n",
    "      \"entity_description\": \"A scientific theory developed by Albert Einstein, describing the laws of physics in relation to observers in different frames of reference.\"\n",
    "    },\n",
    "    {\n",
    "      \"entity_name\": \"Nobel Prize in Physics\",\n",
    "      \"entity_type\": \"Award\",\n",
    "      \"entity_description\": \"A prestigious international award in the field of physics, awarded annually by the Royal Swedish Academy of Sciences.\"\n",
    "    }\n",
    "  ],\n",
    "  \"relationships\": [\n",
    "    {\n",
    "      \"source_entity\": \"Albert Einstein\",\n",
    "      \"target_entity\": \"Theory of Relativity\",\n",
    "      \"relation\": \"developed\",\n",
    "      \"relationship_description\": \"Albert Einstein is the developer of the theory of relativity.\"\n",
    "    },\n",
    "    {\n",
    "      \"source_entity\": \"Albert Einstein\",\n",
    "      \"target_entity\": \"Nobel Prize in Physics\",\n",
    "      \"relation\": \"won\",\n",
    "      \"relationship_description\": \"Albert Einstein won the Nobel Prize in Physics in 1921.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "-Real Data-\n",
    "######################\n",
    "text: {text}\n",
    "######################\n",
    "output:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "265aec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def parse_fn(response_str: str) -> Any:\n",
    "    json_pattern = r\"\\{.*\\}\"\n",
    "    match = re.search(json_pattern, response_str, re.DOTALL)\n",
    "    entities = []\n",
    "    relationships = []\n",
    "    if not match:\n",
    "        return entities, relationships\n",
    "    json_str = match.group(0)\n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "        entities = [\n",
    "            (\n",
    "                entity[\"entity_name\"],\n",
    "                entity[\"entity_type\"],\n",
    "                entity[\"entity_description\"],\n",
    "            )\n",
    "            for entity in data.get(\"entities\", [])\n",
    "        ]\n",
    "        relationships = [\n",
    "            (\n",
    "                relation[\"source_entity\"],\n",
    "                relation[\"target_entity\"],\n",
    "                relation[\"relation\"],\n",
    "                relation[\"relationship_description\"],\n",
    "            )\n",
    "            for relation in data.get(\"relationships\", [])\n",
    "        ]\n",
    "        return entities, relationships\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Error parsing JSON:\", e)\n",
    "        return entities, relationships\n",
    "\n",
    "\n",
    "kg_extractor = GraphRAGExtractor(\n",
    "    llm=llm,\n",
    "    extract_prompt=KG_TRIPLET_EXTRACT_TMPL,\n",
    "    max_paths_per_chunk=2,\n",
    "    parse_fn=parse_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd06077",
   "metadata": {},
   "source": [
    "## 7. Docker Setup And Neo4J setup\n",
    "\n",
    "To launch Neo4j locally, first ensure you have docker installed. Then, you can launch the database with the following docker command.\n",
    "\n",
    "```\n",
    "docker run \\\n",
    "    -p 7474:7474 -p 7687:7687 \\\n",
    "    -v $PWD/data:/data -v $PWD/plugins:/plugins \\\n",
    "    --name neo4j-apoc \\\n",
    "    -e NEO4J_apoc_export_file_enabled=true \\\n",
    "    -e NEO4J_apoc_import_file_enabled=true \\\n",
    "    -e NEO4J_apoc_import_file_use__neo4j__config=true \\\n",
    "    -e NEO4JLABS_PLUGINS=\\[\\\"apoc\\\"\\] \\\n",
    "    neo4j:latest\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1debf552",
   "metadata": {},
   "source": [
    "From here, you can open the db at http://localhost:7474/. On this page, you will be asked to sign in. Use the default username/password of neo4j and neo4j.\n",
    "\n",
    "Once you login for the first time, you will be asked to change the password.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c4eccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
    "\n",
    "# Note: used to be `Neo4jPGStore`\n",
    "graph_store = GraphRAGStore(\n",
    "    username=\"neo4j\", password=\"Salesforce1\", url=\"bolt://localhost:7687\", # database=\"nccn\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f09e366c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting paths from text:   0%|          | 0/263 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting paths from text: 100%|██████████| 263/263 [05:19<00:00,  1.22s/it]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:02<00:00,  1.33it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  7.50it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from llama_index.core import PropertyGraphIndex\n",
    "\n",
    "index = PropertyGraphIndex(\n",
    "    nodes=nodes,\n",
    "    kg_extractors=[kg_extractor],\n",
    "    property_graph_store=graph_store,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20871e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EntityNode(label='Organization', embedding=None, properties={'id': 'National Comprehensive Cancer Network', 'entity_description': 'A not-for-profit alliance of leading cancer centers dedicated to improving the quality, effectiveness, and efficiency of cancer care.', 'triplet_source_id': '25e3bdd1-59cc-4648-830e-956b523515af'}, name='National Comprehensive Cancer Network'),\n",
       " Relation(label='recommends', source_id='National Comprehensive Cancer Network', target_id='Sentinel Lymph Node Biopsy', properties={'triplet_key': 'National Comprehensive Cancer Network|recommends|Sentinel Lymph Node Biopsy', 'extraction_node_id': '0462a27d-e7bd-4bb3-9bf6-a5b5c34ee1be', 'source_snippet': '# NCCN Guidelines Version 4.2025\\n\\n# NCCN Guidelines Index\\n\\n# Invasive Breast Cancer\\n\\n# Table of Contents\\n\\n# Discussion\\n\\n# AXILLARY LYMPH NODE STAGING\\n\\nSLNB should be performed and is the preferred method of axillary lymph node staging if the patient is an appropriate SLNB candidate (BINV-D).\\n\\nIn the absence of definitive data demonstrating superior survival, the performance of axillary staging may be considered optional in patients who have particularly favorable tumors, patients for whom the se', 'provenance_id': 'd22d8c0e114578e7177cdc9cde3ec6821e402e9b823d2d188829ed2d8d1e645f', 'triplet_source_id': '0462a27d-e7bd-4bb3-9bf6-a5b5c34ee1be', 'relationship_description': 'The NCCN provides guidelines recommending sentinel lymph node biopsy as the preferred method for axillary lymph node staging in appropriate patients.'}),\n",
       " EntityNode(label='Medical Procedure', embedding=None, properties={'id': 'Sentinel Lymph Node Biopsy', 'entity_description': 'A surgical procedure used to determine the presence of cancer cells in sentinel lymph nodes, with studies evaluating its safety and contraindications during pregnancy in breast cancer patients.', 'triplet_source_id': '066bb053-16ea-4325-83da-293bce683ca7'}, name='Sentinel Lymph Node Biopsy')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.property_graph_store.get_triplets()[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c958600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'National Comprehensive Cancer Network',\n",
       " 'entity_description': 'A not-for-profit alliance of leading cancer centers dedicated to improving the quality, effectiveness, and efficiency of cancer care.',\n",
       " 'triplet_source_id': '25e3bdd1-59cc-4648-830e-956b523515af'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.property_graph_store.get_triplets()[10][0].properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48eb2c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'triplet_key': 'National Comprehensive Cancer Network|recommends|Sentinel Lymph Node Biopsy',\n",
       " 'extraction_node_id': '0462a27d-e7bd-4bb3-9bf6-a5b5c34ee1be',\n",
       " 'source_snippet': '# NCCN Guidelines Version 4.2025\\n\\n# NCCN Guidelines Index\\n\\n# Invasive Breast Cancer\\n\\n# Table of Contents\\n\\n# Discussion\\n\\n# AXILLARY LYMPH NODE STAGING\\n\\nSLNB should be performed and is the preferred method of axillary lymph node staging if the patient is an appropriate SLNB candidate (BINV-D).\\n\\nIn the absence of definitive data demonstrating superior survival, the performance of axillary staging may be considered optional in patients who have particularly favorable tumors, patients for whom the se',\n",
       " 'provenance_id': 'd22d8c0e114578e7177cdc9cde3ec6821e402e9b823d2d188829ed2d8d1e645f',\n",
       " 'triplet_source_id': '0462a27d-e7bd-4bb3-9bf6-a5b5c34ee1be',\n",
       " 'relationship_description': 'The NCCN provides guidelines recommending sentinel lymph node biopsy as the preferred method for axillary lymph node staging in appropriate patients.'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.property_graph_store.get_triplets()[10][1].properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea0be0",
   "metadata": {},
   "source": [
    "### Build communities\n",
    "\n",
    "This will create communities and summary for each community.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09ccc374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/pmui/SynologyDrive/research/2025/bioreasoning',\n",
       " '/Users/pmui/SynologyDrive/research/2025/bioreasoning/notebooks')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.dirname(notebook_dir)\n",
    "project_root, notebook_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4934cb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading graph store from /Users/pmui/SynologyDrive/research/2025/bioreasoning/data/nccn_communities.json\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "persist_path = os.path.join(project_root, NCCN_COMMUNITIES_CACHE_FILE)\n",
    "print(f\"loading graph store from {persist_path}\", flush=True)\n",
    "graph_store.ensure_communities(persist_path=persist_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb1f63d",
   "metadata": {},
   "source": [
    "## 8. Create Query Engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2c214f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = GraphRAGQueryEngine(\n",
    "    graph_store=index.property_graph_store, \n",
    "    llm=llm,\n",
    "    index=index,\n",
    "    similarity_top_k=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33ff10b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "No relevant knowledge found in cached community summaries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"How best to treat breast cancer for patients with HER2?\")\n",
    "display(Markdown(f\"{response.response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fe1e11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
